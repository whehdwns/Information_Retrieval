{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5fe77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2311f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(sys.argv[1], 'r') as f:\n",
    "#     contents = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb54d97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_file=open('headlines.txt',\"r\")\n",
    "yelp_file = open('yelp.txt', \"r\")\n",
    "\n",
    "headline_content = headline_file.read()\n",
    "yelp_content = yelp_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "677c9a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regextoken = RegexpTokenizer(r'<P ID=\\d+>(.*?)</P>')\n",
    "\n",
    "headline_text_list = regextoken.tokenize(headline_content)\n",
    "yelp_text_list = regextoken.tokenize(yelp_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60768163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "#     lower-case words\n",
    "#     Change short term to long terms for verb.\n",
    "#     remove punctuation\n",
    "#         https://www.geeksforgeeks.org/python-remove-punctuation-from-string/\n",
    "\n",
    "def normalization(word):\n",
    "    word= word.lower()\n",
    "    word = word.replace(\"'re\",' are').replace(\"'m'\", ' am').replace(\"'s\",' is').replace(\"n't\",' not').replace(\"'ve\",' have').replace(\"'d\",' had').replace(\"'ll\",' will')\n",
    "    word = word.replace(\"'\",'')\n",
    "    word  = re.sub(r'[^\\w\\s]', '', word)\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "200a2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_word(listed):\n",
    "    normalized_text = []\n",
    "    collection_frequency = Counter()\n",
    "    document_frequency = Counter()\n",
    "    combined_dictionary ={}\n",
    "    \n",
    "    for i in range(len(listed)):\n",
    "        normalized_text.append(normalization(listed[i]))\n",
    "\n",
    "    for i in range(len(normalized_text)):\n",
    "        tokenized_list= []\n",
    "        for j in normalized_text[i].split():\n",
    "            tokenized_list.append(j)\n",
    "        collection_frequency.update(tokenized_list)\n",
    "        document_frequency.update(set(tokenized_list))\n",
    "        \n",
    "    for i in collection_frequency:\n",
    "        combined_dictionary[i] = [collection_frequency[i], document_frequency[i]]\n",
    "    return normalized_text, collection_frequency, document_frequency , combined_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e36aa59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dictionary_terms_occurence(doc_freq, num):\n",
    "    dictionary_terms =[]\n",
    "    for key, value in doc_freq.items():\n",
    "        if value == num:\n",
    "            dictionary_terms.append(key)\n",
    "    return dictionary_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7404ece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_normalized_list, hd_collection_freq, hd_document_freq, hd_combined_dict = calculate_word(headline_text_list)\n",
    "hd_sorted_list_by_freq = sorted(hd_combined_dict.items(), key=lambda r: r[1][0], reverse=True)\n",
    "hd_dict_occurence = dictionary_terms_occurence(hd_document_freq, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68ddd800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------headline--------\n",
      "Number of paragraph: 500000\n",
      "Number of unique words observed: 174195\n",
      "The total number of words encountered: 4586860\n"
     ]
    }
   ],
   "source": [
    "print('---------headline--------')\n",
    "print('Number of paragraph:', len(hd_normalized_list))\n",
    "print('Number of unique words observed:', len(hd_combined_dict))\n",
    "print('The total number of words encountered:', sum(hd_collection_freq.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bce9577c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 100 most frequent words:  [('to', [118907, 109864]), ('in', [88352, 83815]), ('the', [84420, 72959]), ('of', [76129, 70060]), ('for', [67595, 65773]), ('is', [61706, 56690]), ('and', [55417, 51216]), ('on', [41841, 40822]), ('a', [38071, 35242]), ('with', [31812, 31320]), ('at', [31622, 31124]), ('new', [26830, 26321]), ('2015', [20902, 20554]), ('by', [17339, 16766]), ('as', [16673, 16069]), ('from', [16160, 16021]), ('after', [12967, 12922]), ('us', [11747, 11630]), ('market', [10910, 9766]), ('not', [10358, 10161]), ('over', [9840, 9797]), ('be', [9277, 9181]), ('up', [9101, 9000]), ('announces', [8381, 8376]), ('global', [8109, 8018]), ('you', [7981, 7434]), ('says', [7881, 7867]), ('will', [7875, 7809]), ('man', [7798, 7722]), ('more', [7648, 7546]), ('are', [7406, 7272]), ('your', [7368, 7069]), ('out', [7316, 7267]), ('september', [7187, 7135]), ('first', [7107, 7058]), ('it', [7023, 6760]), ('world', [6963, 6859]), ('how', [6743, 6700]), ('this', [6491, 6435]), ('police', [6484, 6434]), ('day', [6411, 6322]), ('report', [5979, 5908]), ('about', [5730, 5675]), ('week', [5524, 5436]), ('video', [5154, 5096]), ('its', [5110, 5015]), ('that', [5046, 4987]), ('inc', [5034, 4471]), ('has', [5005, 4967]), ('no', [4983, 4770]), ('china', [4941, 4840]), ('an', [4938, 4878]), ('what', [4846, 4779]), ('top', [4813, 4762]), ('into', [4770, 4759]), ('2', [4758, 4678]), ('one', [4742, 4598]), ('against', [4662, 4647]), ('off', [4625, 4562]), ('home', [4599, 4513]), ('have', [4591, 4532]), ('school', [4524, 4423]), ('his', [4523, 4391]), ('group', [4445, 4363]), ('who', [4431, 4310]), ('back', [4419, 4384]), ('state', [4407, 4306]), ('business', [4402, 4299]), ('can', [4273, 4247]), ('research', [4244, 4139]), ('city', [4171, 4075]), ('win', [4169, 4147]), ('best', [4165, 4096]), ('review', [4138, 4122]), ('two', [4134, 4078]), ('industry', [4114, 4077]), ('open', [4111, 4083]), ('get', [4080, 4058]), ('now', [4063, 4047]), ('1', [3957, 3865]), ('10', [3910, 3877]), ('3', [3826, 3785]), ('year', [3825, 3762]), ('million', [3782, 3728]), ('all', [3773, 3728]), ('i', [3763, 3425]), ('5', [3762, 3712]), ('time', [3757, 3696]), ('big', [3726, 3648]), ('launches', [3664, 3662]), ('2016', [3599, 3572]), ('why', [3585, 3561]), ('show', [3582, 3540]), ('set', [3561, 3549]), ('down', [3558, 3537]), ('do', [3511, 3424]), ('news', [3479, 3420]), ('pope', [3467, 3434]), ('help', [3461, 3432]), ('cup', [3451, 3399])]\n"
     ]
    }
   ],
   "source": [
    "print('The 100 most frequent words: ', hd_sorted_list_by_freq[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f153dfbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500th ranked words:  ('agreement', [1195, 1193])\n",
      "1000th ranked words:  ('chance', [682, 681])\n",
      "5000th ranked words:  ('hip', [121, 117])\n"
     ]
    }
   ],
   "source": [
    "print('500th ranked words: ',hd_sorted_list_by_freq[499])\n",
    "print('1000th ranked words: ',hd_sorted_list_by_freq[999])\n",
    "print('5000th ranked words: ',hd_sorted_list_by_freq[4999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c04470bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words that occur in exactly one document: 87439\n",
      "The percentage of the dictionary terms which occur in just one document (%): 50.2\n"
     ]
    }
   ],
   "source": [
    "print('The number of words that occur in exactly one document:', len(hd_dict_occurence))\n",
    "print('The percentage of the dictionary terms which occur in just one document (%):', round((len(hd_dict_occurence)/len(hd_document_freq))*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9dd37b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0ddeab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "yp_normalized_list, yp_collection_freq, yp_document_freq, yp_combined_dict = calculate_word(yelp_text_list)\n",
    "yp_sorted_list_by_freq = sorted(yp_combined_dict.items(), key=lambda r: r[1][0], reverse=True)\n",
    "yp_dict_occurence = dictionary_terms_occurence(yp_document_freq, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eba07240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------yelp--------\n",
      "Number of paragraph: 8892\n",
      "Number of unique words observed: 36972\n",
      "The total number of words encountered: 1285265\n"
     ]
    }
   ],
   "source": [
    "print('---------yelp--------')\n",
    "print('Number of paragraph:', len(yp_normalized_list))\n",
    "print('Number of unique words observed:', len(yp_combined_dict))\n",
    "print('The total number of words encountered:', sum(yp_collection_freq.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f232726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 100 most frequent words:  [('the', [65261, 8530]), ('and', [41324, 8273]), ('i', [36350, 7588]), ('a', [33472, 7881]), ('to', [27966, 7425]), ('is', [25401, 7104]), ('was', [23342, 6041]), ('it', [21331, 6789]), ('of', [19543, 6501]), ('not', [17100, 6516]), ('for', [14782, 6272]), ('in', [13857, 5965]), ('that', [11956, 5081]), ('but', [10988, 5559]), ('with', [10426, 4983]), ('have', [10294, 5207]), ('we', [10261, 3463]), ('my', [10234, 4871]), ('this', [10147, 5389]), ('you', [9824, 4291]), ('they', [9378, 4554]), ('on', [8744, 4709]), ('had', [8079, 4298]), ('food', [7972, 4774]), ('are', [7253, 4032]), ('were', [7066, 3468]), ('good', [6728, 4101]), ('at', [6410, 3853]), ('so', [6347, 3738]), ('place', [6335, 4040]), ('as', [5556, 3006]), ('be', [5384, 3546]), ('there', [5274, 3312]), ('like', [4832, 3192]), ('just', [4516, 3013]), ('if', [4493, 3072]), ('out', [4201, 2899]), ('all', [4122, 2811]), ('here', [4107, 2995]), ('very', [4057, 2694]), ('do', [4055, 2782]), ('me', [4017, 2551]), ('one', [3910, 2723]), ('our', [3730, 1933]), ('get', [3704, 2633]), ('their', [3674, 2324]), ('great', [3667, 2615]), ('or', [3662, 2543]), ('when', [3566, 2542]), ('will', [3546, 2725]), ('service', [3540, 2872]), ('would', [3482, 2392]), ('from', [3454, 2518]), ('time', [3297, 2401]), ('go', [3174, 2464]), ('up', [3107, 2247]), ('really', [3105, 2149]), ('which', [3055, 2105]), ('did', [3033, 2095]), ('some', [3015, 2166]), ('what', [3001, 2215]), ('about', [2985, 2201]), ('back', [2981, 2296]), ('been', [2779, 2127]), ('an', [2752, 2106]), ('order', [2668, 1811]), ('no', [2658, 1916]), ('restaurant', [2574, 1789]), ('ordered', [2569, 1883]), ('chicken', [2549, 1557]), ('only', [2479, 1993]), ('more', [2400, 1871]), ('can', [2358, 1824]), ('your', [2342, 1670]), ('also', [2255, 1744]), ('by', [2246, 1753]), ('too', [2206, 1764]), ('us', [2205, 1362]), ('other', [2178, 1771]), ('im', [2174, 1624]), ('because', [2161, 1670]), ('pizza', [2152, 839]), ('got', [2099, 1543]), ('even', [2060, 1650]), ('he', [2059, 1023]), ('menu', [2014, 1517]), ('little', [1981, 1527]), ('them', [1923, 1455]), ('she', [1921, 893]), ('nice', [1910, 1510]), ('after', [1901, 1493]), ('well', [1858, 1478]), ('could', [1855, 1478]), ('has', [1846, 1517]), ('than', [1844, 1531]), ('pretty', [1765, 1370]), ('came', [1757, 1314]), ('much', [1745, 1446]), ('best', [1744, 1458]), ('always', [1732, 1299])]\n"
     ]
    }
   ],
   "source": [
    "print('The 100 most frequent words: ', yp_sorted_list_by_freq[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5451e25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500th ranked words:  ('orders', [317, 277])\n",
      "1000th ranked words:  ('easily', [130, 127])\n",
      "5000th ranked words:  ('overhead', [11, 10])\n"
     ]
    }
   ],
   "source": [
    "print('500th ranked words: ',yp_sorted_list_by_freq[499])\n",
    "print('1000th ranked words: ',yp_sorted_list_by_freq[999])\n",
    "print('5000th ranked words: ',yp_sorted_list_by_freq[4999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3856862f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of words that occur in exactly one document: 20877\n",
      "The percentage of the dictionary terms which occur in just one document (%): 56.47\n"
     ]
    }
   ],
   "source": [
    "print('The number of words that occur in exactly one document:', len(yp_dict_occurence))\n",
    "print('The percentage of the dictionary terms which occur in just one document (%):', round((len(yp_dict_occurence)/len(yp_document_freq))*100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b656587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_hd_yp =[]\n",
    "for i in hd_sorted_list_by_freq[:100]:\n",
    "    for j in yp_sorted_list_by_freq[:100]:\n",
    "        if i[0] == j[0]:\n",
    "            compare_hd_yp.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a44740a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Between Headlines.txt and Yelp.txt, there are  43  same words.\n",
      "Same words between two txt are ['to', 'in', 'the', 'of', 'for', 'is', 'and', 'on', 'a', 'with', 'at', 'by', 'as', 'from', 'after', 'us', 'not', 'be', 'up', 'you', 'will', 'more', 'are', 'your', 'out', 'it', 'this', 'about', 'that', 'has', 'no', 'an', 'what', 'one', 'have', 'back', 'can', 'best', 'get', 'all', 'i', 'time', 'do']\n"
     ]
    }
   ],
   "source": [
    "compare_hd_yp =[]\n",
    "for i in hd_sorted_list_by_freq[:100]:\n",
    "    for j in yp_sorted_list_by_freq[:100]:\n",
    "        if i[0] == j[0]:\n",
    "            compare_hd_yp.append(i[0])\n",
    "\n",
    "print('Between Headlines.txt and Yelp.txt, there are ', len(compare_hd_yp), ' same words.')\n",
    "print('Same words between two txt are', compare_hd_yp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3b5748b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_hd =[]\n",
    "for i in hd_sorted_list_by_freq[:100]:\n",
    "    if i[0] not in compare_hd_yp:\n",
    "        unique_hd.append(i[0])\n",
    "\n",
    "unique_yp =[]\n",
    "for i in yp_sorted_list_by_freq[:100]:\n",
    "    if i[0] not in compare_hd_yp:\n",
    "        unique_yp.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "597d6d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new', '2015', 'market', 'over', 'announces', 'global', 'says', 'man', 'september', 'first', 'world', 'how', 'police', 'day', 'report', 'week', 'video', 'its', 'inc', 'china', 'top', 'into', '2', 'against', 'off', 'home', 'school', 'his', 'group', 'who', 'state', 'business', 'research', 'city', 'win', 'review', 'two', 'industry', 'open', 'now', '1', '10', '3', 'year', 'million', '5', 'big', 'launches', '2016', 'why', 'show', 'set', 'down', 'news', 'pope', 'help', 'cup']\n"
     ]
    }
   ],
   "source": [
    "print(unique_hd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5a9bf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['was', 'but', 'we', 'my', 'they', 'had', 'food', 'were', 'good', 'so', 'place', 'there', 'like', 'just', 'if', 'here', 'very', 'me', 'our', 'their', 'great', 'or', 'when', 'service', 'would', 'go', 'really', 'which', 'did', 'some', 'been', 'order', 'restaurant', 'ordered', 'chicken', 'only', 'also', 'too', 'other', 'im', 'because', 'pizza', 'got', 'even', 'he', 'menu', 'little', 'them', 'she', 'nice', 'well', 'could', 'than', 'pretty', 'came', 'much', 'always']\n"
     ]
    }
   ],
   "source": [
    "print(unique_yp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
