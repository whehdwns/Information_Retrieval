{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3f40899",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dongj\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import string\n",
    "import time\n",
    "import sys\n",
    "import glob\n",
    "import random\n",
    "import hashlib as hl\n",
    "import tracemalloc\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b35ebc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "testfilesd = glob.glob(path + \"/dataset/duplicatetests/*.tsv\")\n",
    "testfilesk = os.listdir('dataset/twoktests/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45034c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\dongj\\\\Desktop\\\\JHU_Course\\\\Fall_2022\\\\Information_Retrieval\\\\PA\\\\PA_5/dataset/duplicatetests\\\\hundred.tsv',\n",
       " 'C:\\\\Users\\\\dongj\\\\Desktop\\\\JHU_Course\\\\Fall_2022\\\\Information_Retrieval\\\\PA\\\\PA_5/dataset/duplicatetests\\\\hundredk.tsv',\n",
       " 'C:\\\\Users\\\\dongj\\\\Desktop\\\\JHU_Course\\\\Fall_2022\\\\Information_Retrieval\\\\PA\\\\PA_5/dataset/duplicatetests\\\\onek.tsv',\n",
       " 'C:\\\\Users\\\\dongj\\\\Desktop\\\\JHU_Course\\\\Fall_2022\\\\Information_Retrieval\\\\PA\\\\PA_5/dataset/duplicatetests\\\\tenk.tsv',\n",
       " 'C:\\\\Users\\\\dongj\\\\Desktop\\\\JHU_Course\\\\Fall_2022\\\\Information_Retrieval\\\\PA\\\\PA_5/dataset/duplicatetests\\\\thirty.tsv',\n",
       " 'C:\\\\Users\\\\dongj\\\\Desktop\\\\JHU_Course\\\\Fall_2022\\\\Information_Retrieval\\\\PA\\\\PA_5/dataset/duplicatetests\\\\thirtyk.tsv',\n",
       " 'C:\\\\Users\\\\dongj\\\\Desktop\\\\JHU_Course\\\\Fall_2022\\\\Information_Retrieval\\\\PA\\\\PA_5/dataset/duplicatetests\\\\threehundred.tsv',\n",
       " 'C:\\\\Users\\\\dongj\\\\Desktop\\\\JHU_Course\\\\Fall_2022\\\\Information_Retrieval\\\\PA\\\\PA_5/dataset/duplicatetests\\\\threek.tsv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testfilesd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28feb454",
   "metadata": {},
   "outputs": [],
   "source": [
    "thirty_df_orig = pd.read_csv(testfilesd[4],sep='\\t+|\\n',header=None, names=['Context'])\n",
    "hundred_df_orig = pd.read_csv(testfilesd[0],sep='\\t+|\\n',header=None, names=['Context'])\n",
    "threehundred_df_orig = pd.read_csv(testfilesd[6],sep='\\t+|\\n',header=None, names=['Context'])\n",
    "onek_df_orig = pd.read_csv(testfilesd[2],sep='\\t+|\\n',header=None, names=['Context'])\n",
    "threek_df_orig = pd.read_csv(testfilesd[7],sep='\\t+|\\n',header=None, names=['Context'])\n",
    "tenk_df_orig = pd.read_csv(testfilesd[3],sep='\\t+|\\n',header=None, names=['Context'])\n",
    "thirtyk_df_orig = pd.read_csv(testfilesd[5],sep='\\t+|\\n',header=None, names=['Context'])\n",
    "hundredk_df_orig = pd.read_csv(testfilesd[1],sep='\\t+|\\n',header=None, names=['Context'])\n",
    "\n",
    "thirty_df = thirty_df_orig.copy()\n",
    "hundred_df = hundred_df_orig.copy()\n",
    "threehundred_df = threehundred_df_orig.copy()\n",
    "onek_df = onek_df_orig.copy()\n",
    "threek_df = threek_df_orig.copy()\n",
    "tenk_df = tenk_df_orig.copy()\n",
    "thirtyk_df = thirtyk_df_orig.copy()\n",
    "hundredk_df = hundredk_df_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d1b27b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "#     lower-case words\n",
    "#     Change short term to long terms for verb.\n",
    "#     remove punctuation\n",
    "#         https://www.geeksforgeeks.org/python-remove-punctuation-from-string/\n",
    "\n",
    "def normalization(word):\n",
    "    word = word.replace(\"'\",' ')\n",
    "    word = word.replace(\"'re\",' are').replace(\"'m'\", ' am').replace(\"'s\",' is').replace(\"n't\",' not').replace(\"'ve\",' have').replace(\"'d\",' had').replace(\"'ll\",' will')\n",
    "    word  = re.sub(r'[^\\w\\s]', ' ', word)\n",
    "    word = word.translate(str.maketrans('', '', string.punctuation))\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f94cc486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess dataset \n",
    "#   Normalization\n",
    "#   Removed Stop words \n",
    "def preprocess(data):\n",
    "    result = []\n",
    "    for line in data:\n",
    "        word = normalization(line)\n",
    "        word = word.lower().strip().split()\n",
    "        stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "        word = [w for w in word if not w in stopwords]\n",
    "        word = \" \".join(word)\n",
    "        result.append(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d848b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_gram(text, N):\n",
    "    grams_list=set()\n",
    "    text = text.split()\n",
    "    for i in range(len(text)-N+1):\n",
    "        shingle = text[i:i+N]\n",
    "        shingle = ' '.join(shingle)\n",
    "        grams_list.add(shingle)\n",
    "    return grams_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea051df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashing(text):\n",
    "    return int.from_bytes(hl.sha256(text.encode(\"utf-8\")).digest()[:8], 'little') # 64-bit int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea47f309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_random_hash_fn(N):\n",
    "    a = random.randint(N+1,(2**64 - N))\n",
    "    b = random.randint(N,(2**64 - N))\n",
    "    return lambda x: (a * x + b) % N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f184c494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hashes(N, num_hashes):\n",
    "    list_of_hash_fn=[]\n",
    "    for i in range(num_hashes):\n",
    "        list_of_hash_fn.append(make_random_hash_fn(N))\n",
    "    return list_of_hash_fn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48bf6ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingledhash(df, N):\n",
    "    docsAsShingleSets = {}\n",
    "    for i in range(1,len(df)+1):\n",
    "        n_gram_list=N_gram(df[i], N)\n",
    "        shinglesInDoc = set()\n",
    "        for j in range(len(n_gram_list)):\n",
    "            shinglesInDoc.add(hashing(list(n_gram_list)[j]))\n",
    "        docsAsShingleSets[i] = shinglesInDoc\n",
    "    return docsAsShingleSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1af11e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shingles_doc(shingled_documents):\n",
    "    list_of_tuples = []\n",
    "    list_of_documentid =[]\n",
    "    for i in shingled_documents:\n",
    "        list_of_documentid.append(i)\n",
    "        for j in shingled_documents[i]:\n",
    "            list_of_tuples.append((j, i))\n",
    "    list_of_tuples.sort()\n",
    "    return list_of_tuples, list_of_documentid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "354fb09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_minhash_signature(shingled_data, num_hash):\n",
    "    inv_index, docids = shingles_doc(shingled_data)\n",
    "    num_docs = len(docids)\n",
    "    sigmatrix = np.full([num_hash, num_docs], np.inf)  \n",
    "    hash_funcs = make_hashes(len(inv_index), num_hash)\n",
    "    for row, docid in inv_index:\n",
    "        for row1 in range(num_hash):\n",
    "            sigmatrix[row1,docids.index(docid)]=min(sigmatrix[row1,docids.index(docid)],hash_funcs[row1](row))\n",
    "    return sigmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce674f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacard_similarity(id1, id2, minhash_sigmat, docids):\n",
    "    return np.mean(minhash_sigmat[:, docids.index(id1)]==minhash_sigmat[:, docids.index(id2)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f2dff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def near_document(df, sigmin, threshold):\n",
    "    near_docs=[]\n",
    "    for i in range(len(df)):\n",
    "        for j in range(i+1, len(df)):\n",
    "            minhash_similar=jacard_similarity(list(df.keys())[i], list(df.keys())[j], sigmin, list(df.keys()))\n",
    "            minhash_tuples = [list(df.keys())[i], list(df.keys())[j],minhash_similar]\n",
    "            if minhash_similar>=threshold:\n",
    "                near_docs+=[minhash_tuples]\n",
    "    return near_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8dbaadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_output(df, N):\n",
    "    comb_doc=[]\n",
    "    for i in range(1,N+1):\n",
    "        near =[]\n",
    "        for j in df:\n",
    "            if j[0] == i:\n",
    "                near.append(j[0])\n",
    "                near.append(j[1])\n",
    "            else:\n",
    "                continue\n",
    "        comb_doc_list =[]\n",
    "        for k in df:\n",
    "            for g in near:\n",
    "                if k[0]==g:\n",
    "                    comb_doc_list.append(k[0])\n",
    "                    comb_doc_list.append(k[1])\n",
    "        comb_doc.append(comb_doc_list)\n",
    "    sorted_doc =[]\n",
    "    for i in tz3:\n",
    "        if len(i) !=0:\n",
    "            sorted_doc.append(sorted(set(i)))\n",
    "    first_doc=[]\n",
    "    for i in sorted_doc:\n",
    "        for j in i[1:]:\n",
    "            first_doc.append(j)\n",
    "    merge_doc =[]\n",
    "    for i in sorted_doc:\n",
    "        for j in first_doc:\n",
    "            if i[0] ==j:\n",
    "                merge_doc.append(i)\n",
    "    near_ducplicate_list =[]\n",
    "    for i in sorted_doc:\n",
    "        if i not in merge_doc:\n",
    "            near_ducplicate_list.append(i)\n",
    "    return near_ducplicate_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa7fb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def near_duplicate_doc(near_doc, N):\n",
    "    doc_id =[]\n",
    "    for i in near_doc:\n",
    "        for j in i:\n",
    "            doc_id.append(j)  \n",
    "    with open('output/dcho13-'+str(N)+'.txt', 'w') as f:\n",
    "        for k in range(1, N+1):\n",
    "            near =[]\n",
    "            for i in near_doc:\n",
    "                if i[0] == k:\n",
    "                    near.append(i)\n",
    "            if len(near) !=0:\n",
    "                print(*near[0], file=f)\n",
    "            if k in doc_id:\n",
    "                continue\n",
    "            else:\n",
    "                print(k, file=f)\n",
    "    print('output/dcho13-'+str(N)+'.txt is created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6045f221",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "thirty_df['processed_context'] = preprocess(thirty_df['Context'])\n",
    "shingled_list_30= shingledhash(thirty_df['processed_context'], 3)\n",
    "minhash_sigmat_30=make_minhash_signature(shingled_list_30, 200)\n",
    "near_30 = near_document(shingled_list_30, minhash_sigmat_30,0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989b377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_30 = cluster_output(near_30, 30)\n",
    "near_duplicate_doc(output_30,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abebf88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "hundred_df['processed_context'] = preprocess(hundred_df['Context'])\n",
    "shingled_list_100= shingledhash(hundred_df['processed_context'], 3)\n",
    "minhash_sigmat_100=make_minhash_signature(shingled_list_100, 200)\n",
    "near_100 = near_document(shingled_list_100, minhash_sigmat_100, 0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad15b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_100 = cluster_output(near_100, 100)\n",
    "near_duplicate_doc(output_100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6337e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "threehundred_df['processed_context'] = preprocess(threehundred_df['Context'])\n",
    "shingled_list_300= shingledhash(threehundred_df['processed_context'], 3)\n",
    "minhash_sigmat_300=make_minhash_signature(shingled_list_300, 200)\n",
    "near_300 = near_document(shingled_list_300, minhash_sigmat_300, 0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e9756",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_300 = cluster_output(near_300, 300)\n",
    "near_duplicate_doc(output_300,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27661170",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "onek_df['processed_context'] = preprocess(onek_df['Context'])\n",
    "shingled_list_1000= shingledhash(onek_df['processed_context'], 3)\n",
    "minhash_sigmat_1000=make_minhash_signature(shingled_list_1000, 200)\n",
    "near_1000 = near_document(shingled_list_1000, minhash_sigmat_1000, 0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d38f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1000 = cluster_output(near_1000, 1000)\n",
    "near_duplicate_doc(output_1000,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3465ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "testfilesd2= glob.glob(path + \"/dataset/twoktests/*.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94210c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "twok_df_orig = pd.read_csv(testfilesd2[0],sep='\\t+|\\n',header=None, names=['Context'])\n",
    "twok_df = twok_df_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb0ea0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "twok_df['processed_context'] = preprocess(twok_df['Context'])\n",
    "shingled_list_2000= shingledhash(twok_df['processed_context'], 3)\n",
    "minhash_sigmat_2000=make_minhash_signature(shingled_list_2000, 100)\n",
    "near_2000 = near_document(shingled_list_2000, minhash_sigmat_2000, 0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c437e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_2000 = cluster_output(near_2000, 2000)\n",
    "near_duplicate_doc(output_2000,2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee351f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "threek_df['processed_context'] = preprocess(threek_df['Context'])\n",
    "shingled_list_3000= shingledhash(threek_df['processed_context'], 3)\n",
    "minhash_sigmat_3000=make_minhash_signature(shingled_list_3000, 200)\n",
    "near_3000 = near_document(shingled_list_3000, minhash_sigmat_3000, 0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4447d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_3000 = cluster_output(near_3000, 3000)\n",
    "near_duplicate_doc(output_3000,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c18dd6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tenk_df['processed_context'] = preprocess(tenk_df['Context'])\n",
    "shingled_list_10000= shingledhash(tenk_df['processed_context'], 3)\n",
    "minhash_sigmat_10000=make_minhash_signature(shingled_list_10000, 200)\n",
    "near_10000 = near_document(shingled_list_10000, minhash_sigmat_10000, 0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae59dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_10000 = cluster_output(near_10000, 10000)\n",
    "near_duplicate_doc(output_10000,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4670306e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "thirtyk_df['processed_context'] = preprocess(thirtyk_df['Context'])\n",
    "shingled_list_30000= shingledhash(thirtyk_df['processed_context'], 3)\n",
    "minhash_sigmat_30000=make_minhash_signature(shingled_list_30000, 200)\n",
    "near_30000 = near_document(shingled_list_30000, minhash_sigmat_30000, 0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceb7bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_30000 = cluster_output(near_30000, 30000)\n",
    "near_duplicate_doc(output_30000,30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f036ac5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hundredk_df['processed_context'] = preprocess(hundredk_df['Context'])\n",
    "shingled_list_100000= shingledhash(hundredk_df['processed_context'], 3)\n",
    "minhash_sigmat_100000=make_minhash_signature(shingled_list_100000, 200)\n",
    "near_100000 = near_document(shingled_list_100000, minhash_sigmat_100000, 0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcec5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_100000 = cluster_output(near_100000, 100000)\n",
    "near_duplicate_doc(output_100000,100000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
